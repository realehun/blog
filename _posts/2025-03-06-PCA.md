---
title: PCA (Principle Component Analysis) (PENDING)
date: 2025-03-06 12:46:46 +05300
categories: [Computer-Science, Machine-Learning]
tags: [mathematics, computer_science, linear_algebra, compression, machine_learning, ai]     # TAG names should always be lowercase
description: In this post we discuss about PCA and its applications.
math: true
image:
  path: /assets/img/PCA/thumbnail.png
  alt: Genes from 3D space projected onto 2D hyperplane, determined by the top 2 principle components. [Source](http://www.nlpca.org/pca_principal_component_analysis.html)
---

<div class="custom" markdown="1" style="font-family: Verdana">

## 1. Introduction

PCA (Principle Component Analysis) is a technique for dimensionality reduction or put mathematically finding lower dimensional subspaces s.t projection of vectors onto it retain maximum "self" (like minimizing mean square error). To define our problem statement, let $$V$$ be the original vector space and $$U$$ be the output vector space (of lower dimension i.e.). The idea is we have points in $$V$$ space, hopefully correlated in some manner[^1], and we would like to transform (project) them onto vectors of $$U$$ space (i.e. of lower dimension) s.t that correlation is preserved. A simple way to envision it is images (which are nothing but high dimensional vectors in euclidean space). Images of the same objects (like the digit 7) hold some specific region in the vector space, our task while transforming to lower dimensional space would be to - keep the close by vectors close and far off vectors far off, i.e. preserve neighbourhood and have the ability to reconstruct original vectors[^2].

## 2. PCA

A very simple transformation that preserves locality is a [Linear transform](https://en.wikipedia.org/wiki/Linear_map). Elementary algebra will tell this is equivalent to multiplying vectors in $$V$$ by some matrix $$A$$ of dimension $$u \times v$$, where $$v$$ and $$u$$ are dimensions of input and output vectors spaces. Now let us focus on the reconstruction part. Basically given an output vector $$z \in U$$ we would like to get back the original $$x \in V$$ with the least error. I.e. $$\min_{W, z} \mathbb{E}[\Vert x - Wz \Vert]$$[^4], expectation is over all values of $$x$$. This is a 2 fold optimization, we will first find the $$z$$ given the reconstruction matrix $$W$$ (for which we can just care about the subproblem $$\Vert x - Wz \Vert$$ not the whole expectation) and then find the best $$W$$. 

$$z$$ is simple to find, one uses matrix calculus identities[^3] and sets the derivative to 0[^13]. We get $$z = (W^TW)^{-1}W^Tx$$. To procede with $$W$$, we first note a simple identity $$ \min E[\Vert x \Vert]$$ is equivalent to $$\min E[x^Tx]$$ and also equivalent to $$\min E[\text{Tr}(xx^T)]$$[^5]. To simplify notation we write $$z = W^+ x$$, where $$W^+$$ is the Moore-Penrose Pseudoinverse of $$W$$. The equation $$\min \mathbb{E}[\Vert x - WW^+x \Vert]$$ can be simplified as follows -

$$
\begin{align*}
&= \min \mathbb{E}[\text{Tr}((x - WW^+x)(x - WW^+x)^T)] \\
&= \min \mathbb{E}[\text{Tr}((x - WW^+x)(x^T - x^TW^{+^T}W^T))] \\
&= \min \mathbb{E}[\text{Tr}(xx^T - xx^TW^{+^T}W^T - WW^+xx^T + WW^+xx^TW^{+^T}W^T)] \\
&= \min \text{Tr}(\mathbb{E}[(xx^T - xx^TW^{+^T}W^T - WW^+xx^T + WW^+xx^TW^{+^T}W^T]) \\
&= \min \text{Tr}(\Sigma) - \text{Tr}(\Sigma W^{+^T}W^T) - \text{Tr}(WW^+\Sigma) + \text{Tr}(WW^+\Sigma W^{+^T}W^T) \\
&= \min - \text{Tr}(\Sigma W^{+^T}W^T) - \text{Tr}(WW^+\Sigma) + \text{Tr}(WW^+\Sigma W^{+^T}W^T) \\
\end{align*}
$$

Here $$\mathbb{E}[xx^T] = \Sigma$$. Let me explain the little trickeries step by step. First 3 steps are algebra. In the 4th step, we make note that Expectation and trace are interchangable for a matrix[^6]. In step 5, we use lineraity of expectation and identities like: $$\mathbb{E}[AXC] = A\mathbb{E}[X]C$$[^7]. We also replaced the expectation of $$xx^T$$ by $$\Sigma$$, which represents the covariance matrix + $$\mu\mu^T$$[^8]. While applying PCA we usually make sure that image are all aligned and centered correctly[^9], i.e. assume means are 0, so $$\Sigma$$ is just the covariance matrix. In step 6, we removed the constant term as it doesn't afect the minimizing problem. 

Now we will exploit the properties of Moore-Penrose Pseudoinverse, or to assume no prior knowledge, of the matrix $$(W^TW)^{-1}W^T$$, basically we will show that the matrix, $$WW^+$$, is an orthogonal projection[^10] which means it is both idempotent and symmetric. Let us start with idempotence, $$(WW^+)^2 = WW^+WW^+ = WW^+$$ because $$WW^+W = W$$, which follows trivially from definition of $$W^+$$. Symmetric is no different, just take the transpose and remember the 2 properties of matrix transpose: $$(AB)^T = B^TA^T$$ and $$A^{-1^T} = A^{T^{-1}}$$[^11]. One more thing, trace is invariant under cyclic permutation i.e. for 3 matrices it would mean: $$\text{Tr}(ABC) = \text{Tr}(CAB) = \text{Tr}(BCA)$$[^12]. For notation sake let us denote the projection matrix $$WW^+$$ by $$P$$. Above equation becomes -

$$
\begin{align*}
    &= \min - \text{Tr}(\Sigma P) - \text{Tr}(P \Sigma) + \text{Tr}(P \Sigma P) \\
    &= \min - \text{Tr}(P \Sigma) - \text{Tr}(P \Sigma) + \text{Tr}(P^2 \Sigma) \\
    &= \min - \text{Tr}(P \Sigma) \\
\end{align*}
$$

In the last step we use the idempotent property of projection. It is almost over, we would call upon the literature of Linear Algebre second last time: Eigenvalue decomposition theorem tells us that for a real symmetric matrix we have the following decomposition $$A = Q\Lambda Q^T$$, where $$Q$$ is the orthogonal matrix of eigenvectors (each column is the eigenvector) and $$\Lambda$$ is a diagonal matrix whose entries are eigenvalues ($$\lambda_1 \gt \lambda_2 \dots \gt \lambda_n$$, i.e. top to bottom in decreasing order of eigenvalues). Note that eigenvectors in $$Q$$ are also in order of the eigenvalues (corresponding to them i.e.). In our problem $$\Sigma$$ is a real symmetric matrix and thus posits this eigenvalue decomposition. Now as these orthogonal eigenvectors form a basis, we can write each column in $$W$$ as some linear combination of them. In matrix form that would mean $$W = QY$$ where $$Y$$ is a $$v \times u$$ matrix. Substituting above and noting that $$Q^TQ = I$$ (for orthogonal matrix it's transpose is also it's inverse), we get:

$$
\begin{align*}
    &= \min -\text{Tr}[Y(Y^TY)^{-1}Y^T\Lambda]
\end{align*}
$$

The end is very near, one last time we ask the literature of linear algebra for some insight. If we notice carefully we see that the matrix multiplied by $$\Sigma$$ is nothing but an orthogonal projection matrix (its similar to $$WW^+$$, $$Y$$ is also full column rank). 

## Notes

[^1]: Mathematically correlation (Pearson's) refer to $$\rho_{X,Y} = \text{Cov}(X,Y) / (\sigma_X \sigma_Y)$$, where $$\text{Cov}(X,Y)$$ is given by $$\mathbb{E}[(X-\mathbb{E}[X])(Y - \mathbb{E}[Y])]$$. The idea is very simple: If $$X$$ and $$Y$$ are independent this term is 0, which is clear from property of expectation as well as the intutive idea that on average $$X$$ (mean subtracted, same goes from all references of $$X$$ and $$Y$$ in this note) is as much positive as negative for same $$Y$$ resulting in net 0 contribution. When they are not independent, a positive covariance would represent that when $$X$$ is positive, on average $$Y$$ tends to be positive too (and similarly both negative). And negative covariance would mean when $$X$$ is positive (negative) $$Y$$ on average is negative (positive). Correlation is just a normalization of this value, as covariance can be arbitarily small or large (depending on range of $$X$$ and $$Y$$) the value looses quantitative significance. Correlation thus divides it by standard deviation of $$X$$ and $$Y$$, and it turns out $$\rho_{X,Y} \in [-1,1]$$, and the boundary values are attained when $$X = (+-) Y$$, i.e. perfect correlation.

[^2]: I had this doubt about compression. We know there exist a bijection between $$\mathbb{R}$$ and $$\mathbb{R}^n$$, albeit not neighbourhood preserving, but appears to be very space friendly. So why haven't we heard of such a simple compression. There are many reasons, paramount of which is these bijection's aren't space preserving. Bijection is just a ordering, net information hasn't decreased. Consider the example of Hilbert's space filling curves, when they map $$(a,b)$$ to $$c$$, on surface we saved on space of 1-coordinate, but if we look in bit size $$c$$ would have to quite large. Basically bits in form of co-ordinates are being exchanged for bits of $$c$$. Conceretly let both $$a$$ and $$b$$ be 8-bit integers, then $$c$$ will have to be a 16-bit integer to hold all the values (this is finite setting). This begs the question how to compress something then (losslessly)? By exploiting repeating and patterns. For example, consider a message that has all "A" 1000 times. Instead of writing it explicitly we can encode it as "A 1000". [Hufmann algorithm](https://en.wikipedia.org/wiki/Huffman_coding) is one such illustration. This is closed related to information theory, refer to [Shannon's source coding theorem](https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem) . 

[^3]: The [matrix calculus](https://atmos.washington.edu/~dennis/MatrixCalculus.pdf), differentiation and integration of function from multidimensional space to multidimensional space (here differentiation can be a matrix, as that would tell how much changing $$x_1$$ co-ordinates in input changes $$y_1$$ co-ordinates in output), identites are $$\nabla Ax = A$$, $$\nabla x^TAx = x^T(A + A^T)$$, and if $$x$$ is some function of $$z$$ then $$\nabla_{z} x^TAx = x^T (A + A^T) \nabla_z x$$. For our calculation, note that minimizing norm of $$x$$ is same as minimizing norm squared, which can be written as $$x^Tx$$. Thus our minimization problem becomes $$(x-Wz)^T(x-Wz)$$, using our result we get $$2(x - Wz)^T (-W)$$. Equating it to 0, we get $$z = (W^TW)^{-1}W^Tx$$. Note that $$W$$ doesn't have inverse but $$W^TW$$ has if it is full rank[^14] (which we will make sure to check once we have constructed $$W$$). One might be tempted to conclude from the equation $$W^T(x - Wz)$$ that $$x - Wz = 0$$, however one must note that $$W$$ is not invertible so direct inverse doesn't exist, rather one would have to take the Moore-Penrose Pseudoinverse which is nothing but the matrix, $$(W^TW)^{-1}W$$.

[^4]: This is just fancy way of writing least mean square error. It is nice compact way of writing, which puts ideas of simplicity at forefront as you will see in the proof. Linearity of expectation, distribution over product, etc are much more apparent in this format compared to the expanded sum format. This is ubiquitous in modelling optimization problems. Another slightly different way of seeing, consistent with the formuluation, is by having a uniform measure over all possible $$x$$'s and minizing the mean error.

[^5]: The first relation is true because minimizing $$\Vert x \Vert$$ is same as minimizing $$\Vert x^2 \Vert$$. And $$\Vert x^2 \Vert$$ is square of L2 norm and is equivalent to $$x^Tx$$. The last relation when expanded out is also squared L2 norm.

[^6]: Before we explain why, let us first define $$\mathbb{E}[A]$$ where $$A$$ is a matrix. This is a matrix itself $$A^\prime$$ s.t each entry $$a^\prime_{ij} = \mathbb{E}[a_{ij}]$$, i.e. each entry in matrix is just the expecation of that entry in input matrix. We claimed that $$\mathbb{E}[\text{Tr}(A)] = \text{Tr}(\mathbb{E}[A])$$. One can simplify expand both terms and quickly realize these are the same thing.

[^7]: This is a trivial result. To build the intution, consider the term $$\mathbb{E}[AX]$$, expectation is over X. The way to solve this would be to first multiply both the matrices and then take the expectation in each cell. But notice as expectation is linear and $$\mathbb{E}[ax] = a\mathbb{E}[x]$$, one realizes expectation just replaces entries $$x_{ij}$$ by $$\mathbb{E}[x_{ij}]$$, i.e. to visualize we just changed the label $$x$$ to $$\mathbb{E}[x]$$ and nothing more, which would mean this is nothing but multiplication of $$A\mathbb{E}[X]$$. From similar reasoning we can write $$\mathbb{E}[AXB] = A\mathbb{E}[X]B$$ too. To extend our result, even this is true $$\mathbb{E}[AXBX] = A\mathbb{E}[X]B\mathbb{E}[X]$$ given all the elements of $$X$$ are independent (so that $$\mathbb{E}[x_1x_2] = \mathbb{E}[x_1]\mathbb{E}[x_2]$$, and the label replacing logic works flawlessly). 

[^8]: In our case vector $$x$$'s co-ordinates aren't independent, else its pointless, our purpose is to exploit symmetry of alignment / preference in some particular direction, hence they are co-related. So one cannot write $$E[xx^T]$$ with the off-diagonal elements simply as the product of correponding means. To make our point consistent with literature, we define covariance matrix as $$\Sigma = \mathbb{E}[(x-\mu)(x-\mu)^T]$$, where $$\mu$$ is a vector of same dimension s.t each co-ordinate is expectation of corresponding co-ordinate of $$x$$. Here if co-ordinates of $$x$$ are independent, off-diagonal elements are 0 (why?) and the diagonal elements are nothing but variance of respective co-ordinate. When the co-ordinates aren't independent the off-diagonal terms, like say entry for (1,2) = $$\mathbb{E}[(x_1 - \mu_1)(x_2 - \mu_2)]$$ is equal to $$\text{Cov}(x_1, x_2)$$.

[^9]: This is not hard to do. Suppose you have vectors $$x_1, \dots , x_n$$, normalize each by following operation: $$x_i \rightarrow$$ $$\frac{x_i - \mathbb{E}[x] }{\sqrt{\text{Var}(x)}}$$. This normalization ensures 0 mean and 1 variance.

[^10]: A projection is a linear transform s.t repeated application has no effect. This is a simple definition, once we have projected a vector onto some subspace, taking the projection again should return the same vector. I.e. $$Px = P^2x \ \forall x$$, which gives us the mathematically definition of projection, $$P = P^2$$ (Idempotent). Now there is another desireable property for a projection called orthogonality, which is $$\langle Px, x - Px \rangle = 0 \ \forall x$$. This gives $$P^T = P^TP$$. Taking transpose we get the definition for orthogonal projection (along with idempotence) $$P = P^T$$.

[^11]: The proofs are pure algebra. For the former, let $$C = AB$$, we know $$C_{ij}$$ is product of $$i$$th row of $$A$$ and $$j$$th column of $$B$$. And $$C^T_{ij} = C_{ji}$$ equals to $$j$$th row of $$A$$ and $$i$$th column of $$B$$. This product can be written is $$i$$th row of $$B^T$$ (as $$i$$th column of $$B$$ becomes $$i$$th row of $$B^T$$) and $$j$$th column of $$A^T$$. By definition of matrix product, this implies $$C^T = B^TA^T$$. For later, we know $$A^{-1}A = I$$. Taking transpose we get $$A^T A^{-1^T} = I$$, which means $$A^{-1^T}$$ is the inverse of $$A^T$$, i.e. $$A^{T^{-1}} = A^{-1^T}$$.

[^12]: It follows immediately from the observation that for 2 matrices $$\text{Tr}(AB) = \text{Tr}(BA)$$. This is quickly realized upon computation, all the term in trace are of form $$a_{ij}b_{ji}$$. When we switch the order the same "transpose" term form the product, these terms can be thought of as complementary. Once we have this, we can show that $$\text{Tr}(A_1A_2 \dots A_n) = \text{Tr}(A_nA_1 \dots A_{n-1})$$. Define $$M = A_1 \dots A_{n-1}$$, as matrix multiplication is associative we can write our equations as $$\text{Tr}(M A_n) = \text{Tr}(A_nM)$$, which we have already proved. Now using this as the starting point we can show that the same is equal to $$\text{Tr}(A_{n-1}A_n A_1 \dots A_{n-2})$$, and so on for all cyclic permutations.

[^13]: One has to very careful while setting derivative to 0 and hoping for a local minima. Consider the simple case of $$x^TAx$$ whose derivative is $$x^T(A + A^T)$$, this is equal to 0 only when $$x = 0$$. But multivariable calculus will tell you whether the point is local minima, maxima, saddle, or inconclusive depends on the matrix $$A$$. In general, if the hessian is positive definite (all eigenvalues > 0) then it is a local minima, if negative definite then it is a local maxima and if there are eigenvalues both positive and negative it is saddle point. For positive or negative semidefinite ($$\lambda \ge \text{ , } \le 0$$), this test is inconclusive. The reasoning is not too difficult, another way of defining definitiveness of a matrix is by evaluating the function $$x^TAx$$. If this function is always positive (any $$x$$), then the matrix is deemed positive definite. If $$\ge 0$$ positive semidefinite, if always < 0 negative definite. If sometimes positive and sometimes negative indefinite. It can be shown our 2 definitions are equivalent. Using Taylor series we can have a 2nd degree approximation of our function, $$f(y) = f(x)$$ $$+ \nabla f(x) (y-x)$$ $$+ 1/2 (y-x)^T \nabla^2 f(x) (y-x)$$ $$+ \mathcal{O}(\Vert y - x \Vert^3)$$. For a critical point ($$x^*$$ s.t $$\nabla f(x^*) = 0$$), this becomes $$f(y) = f(x^*) +$$ $$1/2 (y-x^*)^T \nabla^2 f(x^*) (y - x^*)$$, we ignored the higher term as for sufficiently lower $$\Vert y - x \Vert$$ it becomes strictly less than our $$1/2 (y-x^*)^T \nabla^2 f(x^*) (y - x^*)$$ term. By definition of positive definitiveness $$f(y) \gt f(x^*)$$, as long as the higher order term can be removed i.e. nearby point only, which is all we care about. Similarly for negative definitiveness $$f(y) \lt f(x^*)$$, and same for indefinite. In our case when $$f(z) = (x - Wz)^T(x - Wz)$$, the $$\nabla f(z) = -2W^T(x-Wz)$$ and $$\nabla^2 f(z) = 2W^TW$$. If $$W$$ is a full column rank, which we will make sure to check, then $$W^TW$$ is positive definite, and subsequently $$z^*$$ s.t $$\nabla f(z^*) = 0$$ is a local minima.

[^14]: A full column rank means all the column vectors are linearly independent. Let $$W$$ be a full column rank matrix, then we will show that $$W^TW$$ is positive definite. Positive definite means $$v^TWv \gt 0 \ \forall v$$, here that would mean, $$v^T W^T W v =$$ $$(Wv)^T(Wv) = $$ $$\Vert Wv \Vert^2$$. This is clearly $$\ge 0$$ and equal to 0 only when $$v = 0$$ because columns of $$W$$ are linearly independent. We also know that a matrix is invertible iff it is non-singular, i.e. determinant is non-zero. But determinant is just the product of all the eigenvalues. As $$W^TW$$ has all positive eigenvalues (alternate definition of positive definite matrix), determinant is $$> 0$$, and thus $$W^TW$$ is invertible too.

</div>
